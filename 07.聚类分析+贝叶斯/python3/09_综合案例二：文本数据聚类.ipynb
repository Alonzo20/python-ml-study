{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载的20新闻数据中的数据类别为: ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
      "3387条数据；4个新闻类别\n"
     ]
    }
   ],
   "source": [
    "categories = [\n",
    "    'alt.atheism',\n",
    "    'talk.religion.misc',\n",
    "    'comp.graphics',\n",
    "    'sci.space',\n",
    "]\n",
    "print('加载的20新闻数据中的数据类别为:',categories)\n",
    "\n",
    "dataset = fetch_20newsgroups(data_home='datas', subset='all', categories=categories,\n",
    "                             shuffle=True, random_state=42)\n",
    "print(\"%d条数据；%d个新闻类别\" % (len(dataset.data), len(dataset.target_names)))\n",
    "\n",
    "labels = dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cluster_k = np.unique(labels).shape[0]\n",
    "features = 2 ** 20\n",
    "components = 5\n",
    "mini_batch_km_batchsize = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hasher1 = HashingVectorizer(n_features=features, stop_words='english', non_negative=True, \n",
    "                            norm=None, binary=False, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b')\n",
    "tt = TfidfTransformer(norm='l2', use_idf=True)\n",
    "hasher2 = HashingVectorizer(n_features=features, stop_words='english', non_negative=False,\n",
    "                            norm='l2', binary=False, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b')\n",
    "tv = TfidfVectorizer(max_df=0.5, max_features=features, min_df=2, stop_words='english', use_idf=True)\n",
    "\n",
    "\n",
    "vectorizers = [\n",
    "    ('hashing&tf-idf', make_pipeline(hasher1, tt), False),\n",
    "    ('hasing', make_pipeline(hasher2), False),\n",
    "    ('tf-idf', make_pipeline(tv), True)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=components)\n",
    "normalizer = Normalizer(copy=False)\n",
    "sn = make_pipeline(svd, normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mbkm = MiniBatchKMeans(n_clusters=target_cluster_k, init='k-means++', n_init=5, \n",
    "                       init_size=10 * mini_batch_km_batchsize, batch_size=mini_batch_km_batchsize)\n",
    "\n",
    "km = KMeans(n_clusters=target_cluster_k, init='k-means++', max_iter=100, n_init=5)\n",
    "\n",
    "cluster_als = [('Mini-Batch-KMeans', mbkm), ('KMeans', km)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================\n",
      "采用'hashing&tf-idf'的方式将文本数据转换为特征矩阵\n",
      "转换消耗时间:0.735s\n",
      "样本数量:3387,特征属性数量:1048576\n",
      "SVD分解及归一化消耗时间:4.638s\n",
      "降维&归一化操作后，样本数量:3387,特征属性数量:5\n",
      "\n",
      "使用算法Mini-Batch-KMeans对数据进行建模操作\n",
      "模型构建消耗时间:0.028s\n",
      "Mini-Batch-KMeans算法效果评估相关系数\n",
      "均一性/同质性: 0.572\n",
      "完整性: 0.598\n",
      "V-measure: 0.585\n",
      "Adjusted Rand-Index(ARI): 0.597\n",
      "轮廓系数: 0.399\n",
      "聚类中心点为: [[ 0.76695718 -0.458312   -0.04362567  0.06973768  0.0616817 ]\n",
      " [ 0.73214048 -0.31031962  0.00814615 -0.40860922  0.36895898]\n",
      " [ 0.85664217  0.22066209 -0.02394716 -0.20907965 -0.24448361]\n",
      " [ 0.6017225   0.33583113  0.15689202  0.26267315  0.20718269]]\n",
      "\n",
      "使用算法KMeans对数据进行建模操作\n",
      "模型构建消耗时间:0.035s\n",
      "KMeans算法效果评估相关系数\n",
      "均一性/同质性: 0.560\n",
      "完整性: 0.588\n",
      "V-measure: 0.574\n",
      "Adjusted Rand-Index(ARI): 0.571\n",
      "轮廓系数: 0.393\n",
      "聚类中心点为: [[ 0.86348422  0.2074284  -0.02752847 -0.20232829 -0.22822292]\n",
      " [ 0.73371358 -0.51746847 -0.05401306  0.11662839  0.04835384]\n",
      " [ 0.75139166 -0.30350512  0.00840286 -0.38096948  0.35089777]\n",
      " [ 0.59149652  0.33016349  0.16872237  0.27187811  0.2143385 ]]\n",
      "\n",
      "\n",
      "============================================\n",
      "采用'hasing'的方式将文本数据转换为特征矩阵\n",
      "转换消耗时间:0.651s\n",
      "样本数量:3387,特征属性数量:1048576\n",
      "SVD分解及归一化消耗时间:4.267s\n",
      "降维&归一化操作后，样本数量:3387,特征属性数量:5\n",
      "\n",
      "使用算法Mini-Batch-KMeans对数据进行建模操作\n",
      "模型构建消耗时间:0.020s\n",
      "Mini-Batch-KMeans算法效果评估相关系数\n",
      "均一性/同质性: 0.287\n",
      "完整性: 0.296\n",
      "V-measure: 0.291\n",
      "Adjusted Rand-Index(ARI): 0.211\n",
      "轮廓系数: 0.309\n",
      "聚类中心点为: [[ 0.76847436  0.35889633  0.133411   -0.02295064 -0.00363145]\n",
      " [ 0.88634103 -0.29104445  0.0475466   0.0388081  -0.03991046]\n",
      " [ 0.70101334  0.13794714 -0.41610306 -0.07286587 -0.40495823]\n",
      " [ 0.6652897   0.05608795 -0.38993321 -0.36132564  0.30811655]]\n",
      "\n",
      "使用算法KMeans对数据进行建模操作\n",
      "模型构建消耗时间:0.034s\n",
      "KMeans算法效果评估相关系数\n",
      "均一性/同质性: 0.294\n",
      "完整性: 0.301\n",
      "V-measure: 0.298\n",
      "Adjusted Rand-Index(ARI): 0.216\n",
      "轮廓系数: 0.320\n",
      "聚类中心点为: [[ 0.89150059 -0.26208757  0.05879471  0.0348818  -0.05263589]\n",
      " [ 0.70313366  0.15633286 -0.41286165 -0.06976892 -0.3883275 ]\n",
      " [ 0.74496841  0.4108257   0.16475763 -0.01649573 -0.00234635]\n",
      " [ 0.68377641  0.03497178 -0.34927612 -0.34418284  0.32340676]]\n",
      "\n",
      "\n",
      "============================================\n",
      "采用'tf-idf'的方式将文本数据转换为特征矩阵\n",
      "转换消耗时间:0.717s\n",
      "样本数量:3387,特征属性数量:24545\n",
      "SVD分解及归一化消耗时间:0.153s\n",
      "降维&归一化操作后，样本数量:3387,特征属性数量:5\n",
      "\n",
      "使用算法Mini-Batch-KMeans对数据进行建模操作\n",
      "模型构建消耗时间:0.027s\n",
      "Mini-Batch-KMeans算法效果评估相关系数\n",
      "均一性/同质性: 0.582\n",
      "完整性: 0.608\n",
      "V-measure: 0.594\n",
      "Adjusted Rand-Index(ARI): 0.596\n",
      "轮廓系数: 0.438\n",
      "聚类中心点为: [[ 0.57694774  0.34730469  0.16083751  0.22678143  0.25081466]\n",
      " [ 0.709016   -0.54789284 -0.02389831  0.15600949  0.03351417]\n",
      " [ 0.85097544  0.22288521 -0.0467521  -0.16979295 -0.27780455]\n",
      " [ 0.7058126  -0.31774478  0.03053817 -0.47251434  0.31650374]]\n",
      "获取文本转换特征矩阵中，各个分类考虑特征属性的前10个feature特征（10个单词）：\n",
      "类别0:\n",
      " com\n",
      " sandvik\n",
      " sgi\n",
      " livesey\n",
      " keith\n",
      " solntze\n",
      " wpd\n",
      " kent\n",
      " jon\n",
      " apple\n",
      "\n",
      "类别1:\n",
      " space\n",
      " henry\n",
      " toronto\n",
      " nasa\n",
      " access\n",
      " digex\n",
      " com\n",
      " pat\n",
      " gov\n",
      " zoo\n",
      "\n",
      "类别2:\n",
      " god\n",
      " people\n",
      " jesus\n",
      " don\n",
      " com\n",
      " say\n",
      " believe\n",
      " think\n",
      " bible\n",
      " just\n",
      "\n",
      "类别3:\n",
      " graphics\n",
      " space\n",
      " image\n",
      " com\n",
      " university\n",
      " nasa\n",
      " posting\n",
      " program\n",
      " images\n",
      " host\n",
      "\n",
      "\n",
      "使用算法KMeans对数据进行建模操作\n",
      "模型构建消耗时间:0.025s\n",
      "KMeans算法效果评估相关系数\n",
      "均一性/同质性: 0.582\n",
      "完整性: 0.608\n",
      "V-measure: 0.595\n",
      "Adjusted Rand-Index(ARI): 0.596\n",
      "轮廓系数: 0.434\n",
      "聚类中心点为: [[ 0.57934127  0.34049344  0.170797    0.22893377  0.25804946]\n",
      " [ 0.85495659  0.21789805 -0.04441688 -0.17010367 -0.26831832]\n",
      " [ 0.70415387 -0.32227806  0.0328176  -0.47422421  0.31707673]\n",
      " [ 0.71235465 -0.54615887 -0.02114668  0.15381868  0.03656189]]\n",
      "获取文本转换特征矩阵中，各个分类考虑特征属性的前10个feature特征（10个单词）：\n",
      "类别0:\n",
      " com\n",
      " sandvik\n",
      " sgi\n",
      " livesey\n",
      " keith\n",
      " solntze\n",
      " wpd\n",
      " jon\n",
      " kent\n",
      " caltech\n",
      "\n",
      "类别1:\n",
      " god\n",
      " people\n",
      " jesus\n",
      " don\n",
      " com\n",
      " say\n",
      " believe\n",
      " think\n",
      " bible\n",
      " just\n",
      "\n",
      "类别2:\n",
      " graphics\n",
      " space\n",
      " image\n",
      " com\n",
      " university\n",
      " nasa\n",
      " posting\n",
      " program\n",
      " images\n",
      " host\n",
      "\n",
      "类别3:\n",
      " space\n",
      " henry\n",
      " toronto\n",
      " nasa\n",
      " access\n",
      " digex\n",
      " com\n",
      " pat\n",
      " gov\n",
      " zoo\n",
      "\n",
      "\n",
      "\n",
      "==================算法完成======================\n"
     ]
    }
   ],
   "source": [
    "for vectorizer_name, vectorizer, can_inverse in vectorizers:\n",
    "    print(\"============================================\")\n",
    "    print(\"采用'%s'的方式将文本数据转换为特征矩阵\" % vectorizer_name)\n",
    "    \n",
    "    t0 = time()\n",
    "    X = vectorizer.fit_transform(dataset.data)\n",
    "    print(\"转换消耗时间:%.3fs\" % (time() - t0))\n",
    "    print(\"样本数量:%d,特征属性数量:%d\" % X.shape)\n",
    "    \n",
    "    t0 = time()\n",
    "    X = sn.fit_transform(X)\n",
    "    print(\"SVD分解及归一化消耗时间:%.3fs\" % (time() - t0))\n",
    "    print(\"降维&归一化操作后，样本数量:%d,特征属性数量:%d\" % X.shape)\n",
    "    \n",
    "    \n",
    "    for cluster_name, cluster_al in cluster_als:\n",
    "        print()\n",
    "        print(\"使用算法%s对数据进行建模操作\" % cluster_name)\n",
    "        t0 = time()\n",
    "        cluster_al.fit(X)\n",
    "        print(\"模型构建消耗时间:%.3fs\" % (time() - t0))\n",
    "        print(\"%s算法效果评估相关系数\" % cluster_name)\n",
    "        print(u\"均一性/同质性: %0.3f\" % metrics.homogeneity_score(labels, cluster_al.labels_))\n",
    "        print(\"完整性: %0.3f\" % metrics.completeness_score(labels, cluster_al.labels_))\n",
    "        print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, cluster_al.labels_))\n",
    "        print(\"Adjusted Rand-Index(ARI): %.3f\" % metrics.adjusted_rand_score(labels, cluster_al.labels_))\n",
    "        print(\"轮廓系数: %0.3f\" % metrics.silhouette_score(X, cluster_al.labels_, sample_size=1000))\n",
    "        print(\"聚类中心点为:\", cluster_al.cluster_centers_)\n",
    "        \n",
    "        if can_inverse:\n",
    "            print(\"获取文本转换特征矩阵中，各个分类考虑特征属性的前10个feature特征（10个单词）：\")\n",
    "            \n",
    "            original_space_centroids = svd.inverse_transform(cluster_al.cluster_centers_)\n",
    "            \n",
    "            order_centroids = original_space_centroids.argsort()[:, ::-1]\n",
    "        \n",
    "            terms = list(vectorizer.named_steps.items())[0][1].get_feature_names()\n",
    "           \n",
    "            for i in range(target_cluster_k):\n",
    "                print(\"类别%d:\" % i,)\n",
    "                for ind in order_centroids[i, :10]:\n",
    "                    print(' %s' % terms[ind],)\n",
    "                print()\n",
    "    print()\n",
    "    print()\n",
    "print(\"==================算法完成======================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
